The authors introduced **TAS500: semantic segmentation dataset for autonomous driving in unstructured environments**, a novel semantic segmentation dataset for autonomous driving in unstructured environments. Research in autonomous driving for unstructured environments suffers from a lack of semantically labeled datasets compared to its urban counterpart. Urban and unstructured outdoor environments are challenging due to the varying lighting and weather conditions during a day and across seasons. TAS500 offers fine-grained vegetation and terrain classes to learn drivable surfaces and natural obstacles in outdoor scenes effectively. We evaluate the performance of modern semantic segmentation models with an additional focus on their efficiency. The authors experiments demonstrate the advantages of fine-grained semantic classes to improve the overall prediction accuracy, especially along the class boundaries. 


## Dataset creation

Semantic scene understanding is a key capability for autonomous robot navigation in real-world environments, but current research in autonomous driving focuses mainly on urban, suburban, and highway scenes. These scenes are considered as structured environments. In terms of their scene statistics, structured environments often provide more explicit object boundaries and contain objects with strong structural regularities. The authors are interested in robot navigation in unstructured environments, such as paths through forests and along fields. Here the occurrence of many semi-transparent classes such as tree foliage and the subtle color difference between vegetation types pose a challenge during image processing. The authors institute Autonomous Systems Technology has developed an autonomous vehicle named MuCAR-3 (Munich Cognitive Autonomous Robot Car) with full drive-by-wire capabilities. MuCAR-3 is equipped with multiple sensors, such as LiDAR, vision systems, and inertial sensors. During test drives with MuCAR-3 in unstructured environments such as forest paths, it turned out that high grass or thin branches were perceived as an obstacle. However, in other cases, there is impassible vegetation like barely visible tree
trunks that MuCAR-3 should avoid during path planning. Apart from a robust detection of passable and impassable vegetation, the domain of robot navigation requires a real-time capability from our semantic segmentation model to be able to react to changes in the environment. The authors studied how a more fine-grained labeling policy affects the overall performance of the semantic segmentation in unstructured environments. That is, they want to distinguish between drivable (grass) and non-drivable vegetation (bush and tree trunk), and detect different kinds of drivable surfaces (asphalt, gravel, soil, sand).

## Dataset description

The authors' goal is to improve understanding of the semantic scene in structured environments led to the development of TAS500 dataset. It contains more than
500 RGB images with pixel-level ground truth annotations for driving in unstructured outdoor environments. The data was collected using the autonomous vehicle
MuCAR-3. The authors vision system is mounted on a multifocal active/reactive camera platform called MarVEye. A camera sensor provides color images at 2.0 MP resolution. They recorded data with a frame rate of 10 Hz and cut off most of the sky and ego vehicle hood from all images. The final images have a resolution of 620 Ã— 2026 px. Their label rate amounts to around 0.1 Hz, and they consequently provide a pixel-wise semantic mask for every hundredth recorded image. The authors provide fine-grained annotations at pixel level, they split up categories such as nature and flat to further subdivide vegetation as well as drivable surfaces. They annotated images from five test drives.

<img src="https://github.com/dataset-ninja/tas500/assets/120389559/d289c527-6940-4f40-87da-9a865498740c" alt="image" width="1000">

<span style="font-size: smaller; font-style: italic;">Number of labeled frames and corresponding duration for the five annotated sequences in TAS500.</span>

The authors pixel-level semantic masks were labeled with a self-developed application, which works similar to the image annotation tool [LabelMe](https://github.com/labelmeai/labelme), but their tool is also capable of annotating 3D point clouds. They use closed polygonal chains to label objects and structures in camera images; exactly one class label was assigned for every pixel. The annotation speed was increased by reusing the object boundaries and totaled approximately 45 min per frame, including a quality control process. The authors labeling policy defines 44 class labels that are categorized into nine groups: _animal_ _construction_, _human_, _object_, _sky_, _terrain_, _vegetation_, _vehicle_, and _void_. Moreover, they want to ensure compatibility with existing datasets. They therefore include most classes from the Cityscapes dataset but exclude rare classes. Thus they only use the classes _car_ and _bus_ from the vehicle category. They map infrequent classes such as rider to the closest corresponding category (here: _person_), while too specific class distinctions such as the classes street light and pole are consolidated into one super-class (here: _pole_). Through this process they arrive at 23 classes. The labeling policy for TAS500 was designed for driving in unstructured environments. Common objects were subdivided if they might require different planning behavior from an
autonomous vehicle. Specifically vegetation classes can be split into drivable (e.g. high grass) and non-drivable vegetation (e.g. bush). The authors fine-grained vegetation annotations allow the distinction between passable (e.g. tree crown) and impassible (e.g. tree trunk) classes within the non-drivable vegetation subcategory. Passable vegetation covers any type of vegetation that an autonomous vehicle could graze during path planning.

<img src="https://github.com/dataset-ninja/tas500/assets/120389559/39854d84-e781-49a3-b6cf-af9bc2183649" alt="image" width="1000">

<span style="font-size: smaller; font-style: italic;">Number of fine-grained pixels (y-axis) per class and their associated category (x-axis).</span>

